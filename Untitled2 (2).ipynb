{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "#!pip install matplotlib-venn\n",
        "#!apt-get -qq install -y libfluidsynth1\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "#!pip install --upgrade tensorflow_federated\n",
        "import tensorflow_federated as tff\n",
        "print(tff.__version__)\n",
        "# '0.20.0'\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Edje0gJ2JuV",
        "outputId": "598741b4-49aa-4def-8cde-fc90dc7d4d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\", line 438, in discover_pjrt_plugins\n",
            "    plugin_module.initialize()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 85, in initialize\n",
            "    options = xla_client.generate_pjrt_gpu_plugin_options()\n",
            "AttributeError: module 'jaxlib.xla_client' has no attribute 'generate_pjrt_gpu_plugin_options'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.87.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tff.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WprZAtIsF9Pp",
        "outputId": "dd51320a-cbbc-4380-fc13-f1a71c0e602e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.1\n",
            "0.87.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load EMNIST data\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(dataset):\n",
        "    def normalize(element):\n",
        "        element['pixels'] = tf.cast(element['pixels'], tf.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
        "        return element\n",
        "    def batch_format_fn(element):\n",
        "        return collections.OrderedDict(\n",
        "            x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "            y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "metadata": {
        "id": "L7pr5iOc8IfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46aec03c-f4ac-470f-ccfc-59207cf49412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading emnist_all.sqlite.lzma: 100%|██████████| 170507172/170507172 [00:37<00:00, 4538401.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create uniform data for all clients\n",
        "def create_uniform_data(client_data, num_clients):\n",
        "    client_ids = client_data.client_ids\n",
        "    total_data = []\n",
        "\n",
        "    for client_id in client_ids:\n",
        "        dataset = client_data.create_tf_dataset_for_client(client_id)\n",
        "        total_data.extend(list(dataset.as_numpy_iterator()))  # Collect all data points\n",
        "\n",
        "    np.random.shuffle(total_data)  # Shuffle the data\n",
        "\n",
        "    # Split the data evenly among the clients\n",
        "    split_data = np.array_split(total_data, num_clients)\n",
        "    return split_data"
      ],
      "metadata": {
        "id": "yywYlcxVUZLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# Assuming these are in your current interactive environment\n",
        "# If they are in files, provide paths.\n",
        "# from ipython_input_18_e31bdb0a7a3f import create_uniform_data\n",
        "# from ipython_input_4_e31bdb0a7a3f import preprocess\n",
        "\n",
        "\n",
        "try:\n",
        "    create_uniform_data\n",
        "    preprocess\n",
        "except NameError:\n",
        "    print(\"The specified functions could not be found. Please check that the files exist and are correctly named.\")\n",
        "    print(\"For example: from file_where_function_is import function_name\")"
      ],
      "metadata": {
        "id": "9CyG8VO04n-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_uniform_data(client_data, num_clients,samples_per_client=500):\n",
        "    # Collect all the data points from all clients\n",
        "    total_data = []\n",
        "    for client_id in client_data.client_ids:\n",
        "        dataset = client_data.create_tf_dataset_for_client(client_id)\n",
        "        total_data.extend(list(dataset.as_numpy_iterator()))\n",
        "\n",
        "    np.random.shuffle(total_data)  # Shuffle the data\n",
        "\n",
        "    # Limit the overall data to a smaller subset\n",
        "    total_data = total_data[:num_clients * samples_per_client]\n",
        "\n",
        "    # Split the data evenly among the clients\n",
        "    split_data = np.array_split(total_data, num_clients)\n",
        "\n",
        "    federated_data = []\n",
        "    for client_dataset in split_data:\n",
        "        # Access elements using the correct key 'pixels' and 'label'\n",
        "        pixels = np.array([data['pixels'] for data in client_dataset])\n",
        "        labels = np.array([data['label'] for data in client_dataset])\n",
        "\n",
        "        # Create a TensorFlow dataset from the numpy arrays\n",
        "        dataset = tf.data.Dataset.from_tensor_slices({\n",
        "            'pixels': pixels,\n",
        "            'label': labels\n",
        "        })\n",
        "\n",
        "        federated_data.append(preprocess(dataset))\n",
        "\n",
        "    return federated_data"
      ],
      "metadata": {
        "id": "83fr66rVCKuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create uniform federated training data\n",
        "federated_train_data = create_uniform_data(emnist_train, NUM_CLIENTS,samples_per_client=500)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAXDtFWq3ejI",
        "outputId": "08dcb043-81e0-42ca-b228-4de5708883f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of client datasets: 10\n",
            "First dataset: <_PrefetchDataset element_spec=OrderedDict([('x', TensorSpec(shape=(None, 784), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model():\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(784,)),\n",
        "        tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "        tf.keras.layers.Softmax(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "_0BUhE496zc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=federated_train_data[0].element_spec,  # Use the correct element_spec\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "diIxGcTHFVfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=0.02),\n",
        "    server_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=1.0))"
      ],
      "metadata": {
        "id": "qEOAiGDqFYtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(iterative_process.initialize.type_signature.formatted_representation())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qagm8xkyFcaU",
        "outputId": "915309a9-9398-44a3-fb47-49a4e6953fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( -> <\n",
            "  global_model_weights=<\n",
            "    trainable=<\n",
            "      float32[784,10],\n",
            "      float32[10]\n",
            "    >,\n",
            "    non_trainable=<>\n",
            "  >,\n",
            "  distributor=<>,\n",
            "  client_work=<\n",
            "    learning_rate=float32\n",
            "  >,\n",
            "  aggregator=<\n",
            "    value_sum_process=<>,\n",
            "    weight_sum_process=<>\n",
            "  >,\n",
            "  finalizer=<\n",
            "    learning_rate=float32\n",
            "  >\n",
            ">@SERVER)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = iterative_process.initialize()"
      ],
      "metadata": {
        "id": "O1msPtkdFhFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round  1, metrics={}'.format(metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOGAPZAKFj47",
        "outputId": "7a944508-f41e-435b-e41f-2683e86c64c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.13604), ('loss', 3.0113468), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_ROUNDS = 11\n",
        "for round_num in range(2, NUM_ROUNDS):\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zcFIy4eFozV",
        "outputId": "8e9c7c73-acf0-4c0b-9556-7ce4af2e5b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.21764), ('loss', 2.506001), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.309), ('loss', 2.1029534), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.39776), ('loss', 1.7994387), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.47668), ('loss', 1.5772904), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  6, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.53968), ('loss', 1.4127856), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  7, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.58804), ('loss', 1.2890452), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  8, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.62252), ('loss', 1.1945726), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  9, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.64972), ('loss', 1.1206677), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 10, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.67304), ('loss', 1.0613796), ('num_examples', 25000), ('num_batches', 1250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_on_test_data(state, test_data):\n",
        "    keras_model = create_keras_model()\n",
        "    model_weights = state.global_model_weights\n",
        "    model_weights.assign_weights_to(keras_model)\n",
        "    test_images, test_labels = test_data\n",
        "    return keras_model, test_images, test_labels"
      ],
      "metadata": {
        "id": "GQNiHmG8MsWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = tf.keras.datasets.mnist.load_data()[1]\n",
        "keras_model, test_images, test_labels = evaluate_model_on_test_data(state, test_data)\n",
        "predictions = np.argmax(keras_model.predict(test_images.reshape(-1, 784)), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX7pyyFIMzmQ",
        "outputId": "4cebeb95-4ab8-4bc1-8afa-f1249f349044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "313/313 [==============================] - 1s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "precision = precision_score(test_labels, predictions, average='weighted')\n",
        "recall = recall_score(test_labels, predictions, average='weighted')\n",
        "f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "cm = confusion_matrix(test_labels, predictions)"
      ],
      "metadata": {
        "id": "dho8e3HrM2dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print('Confusion Matrix:')\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "QQUqnod2M69u",
        "outputId": "c5eee9a7-62a4-45ea-efee-a2d67fdb7809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0425\n",
            "Recall: 0.0973\n",
            "F1 Score: 0.0416\n",
            "Confusion Matrix:\n",
            "[[178 633   0  11   6   0 103   3  29  17]\n",
            " [322 792   0   0   2   0   0  19   0   0]\n",
            " [ 31 780   0   1   9   0   3 165   1  42]\n",
            " [ 17 763  20   0   0   2 188  20   0   0]\n",
            " [  0 919   0   0   0   0   1  28   0  34]\n",
            " [  8 781  18   2   1   0  56  26   0   0]\n",
            " [  7 862   0   0   0   1   1  77   0  10]\n",
            " [ 10 978   0   0   0   0  39   1   0   0]\n",
            " [  0 958   0   0   0   0   1  15   0   0]\n",
            " [  2 997   0   0   1   0   4   4   0   1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiQbsh5-M-Sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}